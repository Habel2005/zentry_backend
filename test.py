from llama_cpp import llama_backend
print(llama_backend())